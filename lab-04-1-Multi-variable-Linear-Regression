Multi-variable linear regression (1)

# data and label
x1 = [ 73.,  93.,  89.,  96.,  73.] #x1의 데이터 
x2 = [ 80.,  88.,  91.,  98.,  66.] #x2의 데이터 
x3 = [ 75.,  93.,  90., 100.,  70.] #x3의 데이터 
Y  = [152., 185., 180., 196., 142.] #Y의 데이터 

# random weights
w1 = tf.Variable(tf.random_normal([1])) #변수w1,w2,w3,b를 만들고 초기값을 1로 설정
w2 = tf.Variable(tf.random_normal([1]))
w3 = tf.Variable(tf.random_normal([1]))
b  = tf.Variable(tf.random_normal([1]))

learning_rate = 0.000001 #learning_rate을 0.000001로 설정

for i in range(1000+1): #학습을 1001번 수행
    # tf.GradientTape() to record the gradient of the cost function
    with tf.GradientTape() as tape: #입력 변수에 대한 값을 tape에 저장
        hypothesis = w1 * x1 +  w2 * x2 + w3 * x3 + b #가섫함수 정의
        cost = tf.reduce_mean(tf.square(hypothesis - Y)) #cost함수 정의
    # calculates the gradients of the cost
    w1_grad, w2_grad, w3_grad, b_grad = tape.gradient(cost, [w1, w2, w3, b])
    ##w1_grad, w2_grad, w3_grad, b_grad를 cost값과 w1, w2, w3, b값으로 정의
    
    # update w1,w2,w3 and b 
    w1.assign_sub(learning_rate * w1_grad) #각각 w(1~3)=w(1~3)-(learning_rate * w(1~3)_grad)
    w2.assign_sub(learning_rate * w2_grad)
    w3.assign_sub(learning_rate * w3_grad)
    b.assign_sub(learning_rate * b_grad) #b=b-(learning_rate * b_grad)

    if i % 50 == 0: #50번 마다 학습값 출력
      print("{:5} | {:12.4f}".format(i, cost.numpy()))
     
Multi-variable linear regression (2)

data = np.array([
    # X1,   X2,    X3,   y
    [ 73.,  80.,  75., 152. ],
    [ 93.,  88.,  93., 185. ],
    [ 89.,  91.,  90., 180. ],
    [ 96.,  98., 100., 196. ],
    [ 73.,  66.,  70., 142. ]
], dtype=np.float32)
# X1, X2, X3, y의 데이터

# slice data
X = data[:, :-1] #X는 마지막 :를 제외한 5행 3열 
y = data[:, [-1]] #y는 마지막 :

W = tf.Variable(tf.random_normal([3, 1])) #x 갯수 3개이므로 W도 3개 만들고 각각 초기값을 1로 설정
b = tf.Variable(tf.random_normal([1])) #b의 초기값 1로 설정

learning_rate = 0.000001 #learning_rate을 0.000001로 설정

# hypothesis, prediction function
def predict(X): #가설함수 정의
    return tf.matmul(X, W) + b

print("epoch | cost") # "epoch | cost" 출력

n_epochs = 2000
for i in range(n_epochs+1): #학습을 2000+1만큼 수행
    # tf.GradientTape() to record the gradient of the cost function
    with tf.GradientTape() as tape: #입력 변수에 대한 값을 tape에 저장
        cost = tf.reduce_mean((tf.square(predict(X) - y))) #cost함수 정의

    # calculates the gradients of the loss
    W_grad, b_grad = tape.gradient(cost, [W, b]) #W_grad와 b_grad를 cost값과 W,b값으로 정의

    # updates parameters (W and b)
    W.assign_sub(learning_rate * W_grad) #W=W-(learning_rate * W_grad)
    b.assign_sub(learning_rate * b_grad) #b=b-(learning_rate * b_grad)
    
    if i % 100 == 0: #50번 마다 학습값 출력
        print("{:5} | {:10.4f}".format(i, cost.numpy()))
